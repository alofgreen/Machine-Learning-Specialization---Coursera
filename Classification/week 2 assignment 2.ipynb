{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:20.796967Z",
     "start_time": "2017-09-23T11:49:17.480049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:21.611599Z",
     "start_time": "2017-09-23T11:49:20.802904Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:21.620689Z",
     "start_time": "2017-09-23T11:49:21.614012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = text.maketrans('','',string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:21.652432Z",
     "start_time": "2017-09-23T11:49:21.624385Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products.review.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:22.817921Z",
     "start_time": "2017-09-23T11:49:21.655314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:49:22.831894Z",
     "start_time": "2017-09-23T11:49:22.820665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_words = pd.read_json('important_words.json')[0]\n",
    "important_words = list(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.274170Z",
     "start_time": "2017-09-23T11:49:22.836946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.293592Z",
     "start_time": "2017-09-23T11:52:23.276660Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_indic = open('module-4-assignment-train-idx.json','r')\n",
    "train_indic=list(train_indic)[0]\n",
    "train_indic=train_indic.split(',')\n",
    "train_indic[0]=' 0'\n",
    "train_indic[-1]=' 53070'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.329322Z",
     "start_time": "2017-09-23T11:52:23.296679Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 0', ' 1', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 10', ' 11']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indic[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.355043Z",
     "start_time": "2017-09-23T11:52:23.332730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_indic = open('module-4-assignment-validation-idx.json','r')\n",
    "val_indic = list(val_indic)[0]\n",
    "val_indic = val_indic.split(',')\n",
    "val_indic[0] = ' 2'\n",
    "val_indic[-1]=' 53071'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.737398Z",
     "start_time": "2017-09-23T11:52:23.357881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_list=[]\n",
    "for line in train_indic:\n",
    "    t = [int(x.strip()) for x in line.split(',')]\n",
    "    train_list.append(t[0])\n",
    "    \n",
    "train_data = products.iloc[train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.801322Z",
     "start_time": "2017-09-23T11:52:23.739860Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_list=[]\n",
    "for line in val_indic:\n",
    "    t = [int(x.strip()) for x in line.split(',')]\n",
    "    val_list.append(t[0])\n",
    "    \n",
    "val_data = products.iloc[val_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data frame to multi-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:23.813227Z",
     "start_time": "2017-09-23T11:52:23.803798Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(df, features, label):\n",
    "    df['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = df[features]\n",
    "    feature_matrix = features_frame.as_matrix()\n",
    "    label_sarray = df[label]\n",
    "    label_array = label_sarray.as_matrix()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:24.120441Z",
     "start_time": "2017-09-23T11:52:23.816208Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andy/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(val_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on logistic regression with no L2 penalty assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T11:52:24.130596Z",
     "start_time": "2017-09-23T11:52:24.123139Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "\n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = np.exp(score)\n",
    "    predictions = predictions+1\n",
    "    predictions = 1/predictions\n",
    "    \n",
    "    # return predictions\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:34.380343Z",
     "start_time": "2017-09-23T12:16:34.373023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    \n",
    "    # Compute the dot product of errors and feature\n",
    "    ## YOUR CODE HERE\n",
    "    derivative = np.dot(errors,feature)\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        derivative = derivative - (2 * l2_penalty * coefficient)\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:35.478816Z",
     "start_time": "2017-09-23T12:16:35.475363Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept, which is the only constant term, was not regularized\n"
     ]
    }
   ],
   "source": [
    "print('The intercept, which is the only constant term, was not regularized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:35.784948Z",
     "start_time": "2017-09-23T12:16:35.777790Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:36.503929Z",
     "start_time": "2017-09-23T12:16:36.498332Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the term with L2 regulatization decreases ll(w)\n"
     ]
    }
   ],
   "source": [
    "print('the term with L2 regulatization decreases ll(w)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:37.391352Z",
     "start_time": "2017-09-23T12:16:37.366963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix,coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(errors, \n",
    "                                                    feature_matrix[:,j], \n",
    "                                                    coefficients[j], \n",
    "                                                    l2_penalty, \n",
    "                                                    is_intercept)\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] = (step_size*derivative)\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:16:38.185427Z",
     "start_time": "2017-09-23T12:16:38.180986Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix_train\n",
    "sentiment = sentiment_train\n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:17:12.721604Z",
     "start_time": "2017-09-23T12:16:38.850331Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29929.17761132\n",
      "iteration   1: log likelihood of observed labels = -29188.11956629\n",
      "iteration   2: log likelihood of observed labels = -29852.05592077\n",
      "iteration   3: log likelihood of observed labels = -29203.52469145\n",
      "iteration   4: log likelihood of observed labels = -29780.66513825\n",
      "iteration   5: log likelihood of observed labels = -29221.12346810\n",
      "iteration   6: log likelihood of observed labels = -29722.05541657\n",
      "iteration   7: log likelihood of observed labels = -29239.22037983\n",
      "iteration   8: log likelihood of observed labels = -29673.66339460\n",
      "iteration   9: log likelihood of observed labels = -29257.03052540\n",
      "iteration  10: log likelihood of observed labels = -29633.54031020\n",
      "iteration  11: log likelihood of observed labels = -29274.03445118\n",
      "iteration  12: log likelihood of observed labels = -29600.14555481\n",
      "iteration  13: log likelihood of observed labels = -29289.92503339\n",
      "iteration  14: log likelihood of observed labels = -29572.24974199\n",
      "iteration  15: log likelihood of observed labels = -29304.54432844\n",
      "iteration  20: log likelihood of observed labels = -29512.62510234\n",
      "iteration  30: log likelihood of observed labels = -29460.69262654\n",
      "iteration  40: log likelihood of observed labels = -29437.18124423\n",
      "iteration  50: log likelihood of observed labels = -29426.21634304\n",
      "iteration  60: log likelihood of observed labels = -29421.02304022\n",
      "iteration  70: log likelihood of observed labels = -29418.54424903\n",
      "iteration  80: log likelihood of observed labels = -29417.35661706\n",
      "iteration  90: log likelihood of observed labels = -29416.78655455\n",
      "iteration 100: log likelihood of observed labels = -29416.51268181\n",
      "iteration 200: log likelihood of observed labels = -29416.25934036\n",
      "iteration 300: log likelihood of observed labels = -29416.25917308\n",
      "iteration 400: log likelihood of observed labels = -29416.25917297\n",
      "iteration 500: log likelihood of observed labels = -29416.25917297\n"
     ]
    }
   ],
   "source": [
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                     sentiment=sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6,\n",
    "                                                     max_iter=501,\n",
    "                                                     l2_penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:18:25.204952Z",
     "start_time": "2017-09-23T12:17:51.986414Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29929.26094613\n",
      "iteration   1: log likelihood of observed labels = -29188.12483091\n",
      "iteration   2: log likelihood of observed labels = -29852.17350117\n",
      "iteration   3: log likelihood of observed labels = -29203.51926031\n",
      "iteration   4: log likelihood of observed labels = -29780.79845510\n",
      "iteration   5: log likelihood of observed labels = -29221.10810160\n",
      "iteration   6: log likelihood of observed labels = -29722.19626108\n",
      "iteration   7: log likelihood of observed labels = -29239.19628563\n",
      "iteration   8: log likelihood of observed labels = -29673.80620094\n",
      "iteration   9: log likelihood of observed labels = -29256.99945120\n",
      "iteration  10: log likelihood of observed labels = -29633.68135364\n",
      "iteration  11: log likelihood of observed labels = -29273.99829201\n",
      "iteration  12: log likelihood of observed labels = -29600.28241872\n",
      "iteration  13: log likelihood of observed labels = -29289.88560812\n",
      "iteration  14: log likelihood of observed labels = -29572.38093030\n",
      "iteration  15: log likelihood of observed labels = -29304.50326250\n",
      "iteration  20: log likelihood of observed labels = -29512.73578014\n",
      "iteration  30: log likelihood of observed labels = -29460.77157505\n",
      "iteration  40: log likelihood of observed labels = -29437.23779588\n",
      "iteration  50: log likelihood of observed labels = -29426.25867588\n",
      "iteration  60: log likelihood of observed labels = -29421.05680943\n",
      "iteration  70: log likelihood of observed labels = -29418.57303953\n",
      "iteration  80: log likelihood of observed labels = -29417.38258901\n",
      "iteration  90: log likelihood of observed labels = -29416.81096380\n",
      "iteration 100: log likelihood of observed labels = -29416.53623895\n",
      "iteration 200: log likelihood of observed labels = -29416.28192851\n",
      "iteration 300: log likelihood of observed labels = -29416.28175996\n",
      "iteration 400: log likelihood of observed labels = -29416.28175985\n",
      "iteration 500: log likelihood of observed labels = -29416.28175985\n"
     ]
    }
   ],
   "source": [
    "coefficients_4_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                     sentiment=sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6,\n",
    "                                                     max_iter=501,\n",
    "                                                     l2_penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:20:48.122760Z",
     "start_time": "2017-09-23T12:20:11.098641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29929.38594835\n",
      "iteration   1: log likelihood of observed labels = -29188.13272931\n",
      "iteration   2: log likelihood of observed labels = -29852.34989537\n",
      "iteration   3: log likelihood of observed labels = -29203.51111524\n",
      "iteration   4: log likelihood of observed labels = -29780.99848184\n",
      "iteration   5: log likelihood of observed labels = -29221.08505051\n",
      "iteration   6: log likelihood of observed labels = -29722.40760506\n",
      "iteration   7: log likelihood of observed labels = -29239.16013628\n",
      "iteration   8: log likelihood of observed labels = -29674.02050972\n",
      "iteration   9: log likelihood of observed labels = -29256.95282221\n",
      "iteration  10: log likelihood of observed labels = -29633.89303580\n",
      "iteration  11: log likelihood of observed labels = -29273.94402408\n",
      "iteration  12: log likelihood of observed labels = -29600.48784498\n",
      "iteration  13: log likelihood of observed labels = -29289.82642882\n",
      "iteration  14: log likelihood of observed labels = -29572.57785255\n",
      "iteration  15: log likelihood of observed labels = -29304.44161010\n",
      "iteration  20: log likelihood of observed labels = -29512.90194619\n",
      "iteration  30: log likelihood of observed labels = -29460.89012841\n",
      "iteration  40: log likelihood of observed labels = -29437.32272206\n",
      "iteration  50: log likelihood of observed labels = -29426.32224354\n",
      "iteration  60: log likelihood of observed labels = -29421.10750747\n",
      "iteration  70: log likelihood of observed labels = -29418.61625199\n",
      "iteration  80: log likelihood of observed labels = -29417.42156174\n",
      "iteration  90: log likelihood of observed labels = -29416.84758477\n",
      "iteration 100: log likelihood of observed labels = -29416.57157695\n",
      "iteration 200: log likelihood of observed labels = -29416.31580610\n",
      "iteration 300: log likelihood of observed labels = -29416.31563564\n",
      "iteration 400: log likelihood of observed labels = -29416.31563553\n",
      "iteration 500: log likelihood of observed labels = -29416.31563553\n"
     ]
    }
   ],
   "source": [
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                     sentiment=sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6,\n",
    "                                                     max_iter=501,\n",
    "                                                     l2_penalty=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:21:22.615482Z",
     "start_time": "2017-09-23T12:20:48.125195Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29931.26098163\n",
      "iteration   1: log likelihood of observed labels = -29188.25141894\n",
      "iteration   2: log likelihood of observed labels = -29854.99921418\n",
      "iteration   3: log likelihood of observed labels = -29203.38918180\n",
      "iteration   4: log likelihood of observed labels = -29784.00631000\n",
      "iteration   5: log likelihood of observed labels = -29220.73910678\n",
      "iteration   6: log likelihood of observed labels = -29725.58892093\n",
      "iteration   7: log likelihood of observed labels = -29238.61675684\n",
      "iteration   8: log likelihood of observed labels = -29677.24949776\n",
      "iteration   9: log likelihood of observed labels = -29256.25086169\n",
      "iteration  10: log likelihood of observed labels = -29637.08520057\n",
      "iteration  11: log likelihood of observed labels = -29273.12582626\n",
      "iteration  12: log likelihood of observed labels = -29603.58812318\n",
      "iteration  13: log likelihood of observed labels = -29288.93279352\n",
      "iteration  14: log likelihood of observed labels = -29575.55194828\n",
      "iteration  15: log likelihood of observed labels = -29303.50912681\n",
      "iteration  20: log likelihood of observed labels = -29515.41612482\n",
      "iteration  30: log likelihood of observed labels = -29462.68745240\n",
      "iteration  40: log likelihood of observed labels = -29438.61103618\n",
      "iteration  50: log likelihood of observed labels = -29427.28578428\n",
      "iteration  60: log likelihood of observed labels = -29421.87448536\n",
      "iteration  70: log likelihood of observed labels = -29419.26838692\n",
      "iteration  80: log likelihood of observed labels = -29418.00835638\n",
      "iteration  90: log likelihood of observed labels = -29417.39796988\n",
      "iteration 100: log likelihood of observed labels = -29417.10200837\n",
      "iteration 200: log likelihood of observed labels = -29416.82330579\n",
      "iteration 300: log likelihood of observed labels = -29416.82310387\n",
      "iteration 400: log likelihood of observed labels = -29416.82310373\n",
      "iteration 500: log likelihood of observed labels = -29416.82310373\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                       sentiment=sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6,\n",
    "                                                       max_iter=501,\n",
    "                                                       l2_penalty=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-23T12:21:55.661868Z",
     "start_time": "2017-09-23T12:21:22.617944Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29950.01131442\n",
      "iteration   1: log likelihood of observed labels = -29189.46219907\n",
      "iteration   2: log likelihood of observed labels = -29881.84895458\n",
      "iteration   3: log likelihood of observed labels = -29202.20232037\n",
      "iteration   4: log likelihood of observed labels = -29814.86961841\n",
      "iteration   5: log likelihood of observed labels = -29217.27565791\n",
      "iteration   6: log likelihood of observed labels = -29758.59204146\n",
      "iteration   7: log likelihood of observed labels = -29233.08582811\n",
      "iteration   8: log likelihood of observed labels = -29711.08397283\n",
      "iteration   9: log likelihood of observed labels = -29248.99448874\n",
      "iteration  10: log likelihood of observed labels = -29670.84363801\n",
      "iteration  11: log likelihood of observed labels = -29264.53607156\n",
      "iteration  12: log likelihood of observed labels = -29636.65553552\n",
      "iteration  13: log likelihood of observed labels = -29279.40169674\n",
      "iteration  14: log likelihood of observed labels = -29607.52475555\n",
      "iteration  15: log likelihood of observed labels = -29293.40011146\n",
      "iteration  20: log likelihood of observed labels = -29542.99673079\n",
      "iteration  30: log likelihood of observed labels = -29482.86724267\n",
      "iteration  40: log likelihood of observed labels = -29453.21897662\n",
      "iteration  50: log likelihood of observed labels = -29438.16112949\n",
      "iteration  60: log likelihood of observed labels = -29430.38101411\n",
      "iteration  70: log likelihood of observed labels = -29426.32262179\n",
      "iteration  80: log likelihood of observed labels = -29424.19462903\n",
      "iteration  90: log likelihood of observed labels = -29423.07573282\n",
      "iteration 100: log likelihood of observed labels = -29422.48655108\n",
      "iteration 200: log likelihood of observed labels = -29421.83082001\n",
      "iteration 300: log likelihood of observed labels = -29421.82972800\n",
      "iteration 400: log likelihood of observed labels = -29421.82972618\n",
      "iteration 500: log likelihood of observed labels = -29421.82972617\n"
     ]
    }
   ],
   "source": [
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                       sentiment=sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6,\n",
    "                                                       max_iter=501,\n",
    "                                                       l2_penalty=1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-23T21:38:58.052Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients_1e5_penalty = logistic_regression_with_L2(feature_matrix=feature_matrix_train,\n",
    "                                                     sentiment=sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6,\n",
    "                                                     max_iter=501,\n",
    "                                                     l2_penalty=1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:44:51.125165Z",
     "start_time": "2017-09-21T21:44:51.105668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table=pd.DataFrame(coeff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:47:18.264781Z",
     "start_time": "2017-09-21T21:47:18.259736Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_list[0]==coeff_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:46:15.968592Z",
     "start_time": "2017-09-21T21:46:15.958543Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.14328461e-02,   1.89516861e-02,   1.76979430e-02,\n",
       "         2.14748014e-02,   1.71123125e-02,   1.50423422e-02,\n",
       "         9.67044801e-03,   1.25373996e-02,   1.65720397e-02,\n",
       "         1.56582526e-02,   8.80568503e-03,   1.31426372e-02,\n",
       "         1.30160618e-02,   7.80779853e-03,   8.34282162e-03,\n",
       "         8.68843774e-03,   9.93484119e-03,   7.11301037e-03,\n",
       "         8.34669847e-03,   7.48246928e-03,   8.24212621e-03,\n",
       "         9.49902111e-03,   7.39405781e-03,   1.12811543e-02,\n",
       "         5.00372133e-03,   7.41498223e-03,   7.37194977e-03,\n",
       "         6.44129177e-03,   6.83237032e-03,   3.52772498e-03,\n",
       "         6.76589993e-03,   8.19826140e-03,   4.75591706e-03,\n",
       "         3.58255875e-03,   7.76056886e-03,   6.71809371e-03,\n",
       "         5.52899564e-03,   4.04905308e-03,   5.15888488e-03,\n",
       "         4.99594935e-03,   4.81042926e-03,   5.64431261e-03,\n",
       "         4.20481846e-03,   4.93911716e-03,   4.68359352e-03,\n",
       "         5.28094755e-03,   4.19739564e-03,   2.78524451e-03,\n",
       "         6.29470376e-03,   4.15694872e-03,   3.59916281e-03,\n",
       "         4.94627628e-03,   5.10753028e-03,   4.48773981e-03,\n",
       "         3.32376847e-03,   5.22789955e-03,   5.54178172e-03,\n",
       "         4.18966727e-03,   4.40753317e-03,   4.84609082e-03,\n",
       "         4.11565970e-03,   3.59976017e-03,   4.45840320e-03,\n",
       "         4.22302055e-03,   3.85583132e-03,   4.83819162e-03,\n",
       "         4.83222536e-03,   2.41490171e-03,   3.95835670e-03,\n",
       "         2.72896933e-03,   2.97883608e-03,   4.04420753e-03,\n",
       "         4.23665397e-03,   3.47334877e-03,   3.59085357e-03,\n",
       "         4.77275684e-03,   4.34011783e-03,   4.02485733e-03,\n",
       "         2.04688269e-03,   3.53182178e-03,   2.94358996e-03,\n",
       "         3.81580113e-03,   2.66860005e-03,   5.00935157e-03,\n",
       "         4.12257945e-03,   4.18964312e-03,   3.41240514e-03,\n",
       "         3.51059725e-03,   4.14376920e-03,   3.34184756e-03,\n",
       "         3.35325740e-03,   4.38412917e-03,   2.60970823e-03,\n",
       "         2.96690132e-03,   2.04000258e-03,   3.70945659e-03,\n",
       "         2.94823568e-03,   6.63210379e-05,   8.28994284e-04,\n",
       "         1.32047685e-03,   7.23579639e-04,   1.92212367e-03,\n",
       "         1.81916737e-03,   1.86416343e-03,   1.34578624e-03,\n",
       "         5.58374474e-04,  -9.27339787e-04,   1.29371974e-03,\n",
       "         1.82605752e-03,   1.78537316e-03,   1.63297106e-03,\n",
       "         2.87600181e-03,   2.22022110e-03,  -7.36948096e-04,\n",
       "        -1.13264866e-03,   2.19743432e-04,   2.69915655e-03,\n",
       "         2.21962908e-03,   2.31464434e-03,   1.82478991e-03,\n",
       "         2.45703150e-03,   2.11845662e-03,   9.78260534e-04,\n",
       "         1.02579701e-03,   2.77705658e-03,   1.66955826e-03,\n",
       "         2.62150224e-03,   2.85014794e-03,   3.06255098e-03,\n",
       "         2.27820794e-03,   1.18270370e-03,   1.29052589e-03,\n",
       "         1.36318846e-03,   1.65032432e-03,   1.25023845e-04,\n",
       "         1.14083475e-03,   2.37886679e-03,   1.84871279e-03,\n",
       "         2.14073866e-03,   7.25366632e-04,   2.10731131e-04,\n",
       "         1.40847917e-03,   2.97787496e-03,   2.87713460e-03,\n",
       "         2.94564802e-03,   3.08174782e-03,   1.25783460e-03,\n",
       "         3.17051585e-03,   1.19751416e-03,   1.80621833e-03,\n",
       "         1.72510961e-03,   2.46863920e-03,   1.93314941e-03,\n",
       "         3.12862927e-03,   2.95935585e-03,   1.51568536e-03,\n",
       "         4.57632114e-04,   5.95988913e-04,   2.85390516e-03,\n",
       "         4.14539062e-04,   3.86725099e-04,   1.96559458e-03,\n",
       "         2.92246305e-03,   1.71305326e-03,   1.28112947e-03,\n",
       "         1.02030794e-03,   1.32299506e-03,  -1.37881652e-04,\n",
       "         2.05887951e-03,  -5.48362525e-04,   1.33424904e-04,\n",
       "         1.40884635e-03,  -4.16755632e-04,   2.87547745e-03,\n",
       "         7.90611109e-04,   9.12370152e-04,   1.84320862e-04,\n",
       "         1.51847018e-03,   9.66194188e-04,   1.09956776e-03,\n",
       "         8.10045071e-04,   1.42841752e-05,   2.05747520e-03,\n",
       "        -1.80761021e-04,   2.61507522e-03,   8.30853284e-04,\n",
       "         1.72067375e-04,   2.90605935e-03,   1.31180366e-03,\n",
       "         2.47282135e-03,   3.94938272e-05,   1.05839376e-03,\n",
       "         1.94008435e-03,   5.70959273e-04])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:45:23.726661Z",
     "start_time": "2017-09-21T21:45:23.706357Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.015658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.013016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.009935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.007113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.007482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.011281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.007415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.007372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.006832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.001323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.002059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.002875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.001518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.000810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.000831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.002906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.002473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.001940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.071433\n",
       "1    0.018952\n",
       "2    0.017698\n",
       "3    0.021475\n",
       "4    0.017112\n",
       "5    0.015042\n",
       "6    0.009670\n",
       "7    0.012537\n",
       "8    0.016572\n",
       "9    0.015658\n",
       "10   0.008806\n",
       "11   0.013143\n",
       "12   0.013016\n",
       "13   0.007808\n",
       "14   0.008343\n",
       "15   0.008688\n",
       "16   0.009935\n",
       "17   0.007113\n",
       "18   0.008347\n",
       "19   0.007482\n",
       "20   0.008242\n",
       "21   0.009499\n",
       "22   0.007394\n",
       "23   0.011281\n",
       "24   0.005004\n",
       "25   0.007415\n",
       "26   0.007372\n",
       "27   0.006441\n",
       "28   0.006832\n",
       "29   0.003528\n",
       "..        ...\n",
       "164  0.001281\n",
       "165  0.001020\n",
       "166  0.001323\n",
       "167 -0.000138\n",
       "168  0.002059\n",
       "169 -0.000548\n",
       "170  0.000133\n",
       "171  0.001409\n",
       "172 -0.000417\n",
       "173  0.002875\n",
       "174  0.000791\n",
       "175  0.000912\n",
       "176  0.000184\n",
       "177  0.001518\n",
       "178  0.000966\n",
       "179  0.001100\n",
       "180  0.000810\n",
       "181  0.000014\n",
       "182  0.002057\n",
       "183 -0.000181\n",
       "184  0.002615\n",
       "185  0.000831\n",
       "186  0.000172\n",
       "187  0.002906\n",
       "188  0.001312\n",
       "189  0.002473\n",
       "190  0.000039\n",
       "191  0.001058\n",
       "192  0.001940\n",
       "193  0.000571\n",
       "\n",
       "[194 rows x 1 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.DataFrame(coeff_list[0])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:33:12.474598Z",
     "start_time": "2017-09-21T21:33:12.465999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_coeff_tuples_lst=[]\n",
    "for x in range(len(coeff_list)):\n",
    "    coeffs = list(coeff_list[x][1:]) # exclude intercept\n",
    "    word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coeff_list[x])]\n",
    "    word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)\n",
    "    word_coeff_tuples_lst.append(word_coefficient_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:36:17.970717Z",
     "start_time": "2017-09-21T21:36:17.966866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_words=word_coeff_tuples_lst[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:36:18.618315Z",
     "start_time": "2017-09-21T21:36:18.614998Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_words=sorted(word_coeff_tuples_lst[0], key=lambda x:x[1], reverse=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T21:36:59.196959Z",
     "start_time": "2017-09-21T21:36:59.118811Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e08a68c8a124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmake_coefficient_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_penalty_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'table' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
    "    cmap_positive = plt.get_cmap('Reds')\n",
    "    cmap_negative = plt.get_cmap('Blues')\n",
    "    \n",
    "    xx = l2_penalty_list\n",
    "    plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "    \n",
    "    table_positive_words = table[table['word'].isin(positive_words)]\n",
    "    table_negative_words = table[table['word'].isin(negative_words)]\n",
    "    del table_positive_words['word']\n",
    "    del table_negative_words['word']\n",
    "    \n",
    "    for i in xrange(len(positive_words)):\n",
    "        color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_positive_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    for i in xrange(len(negative_words)):\n",
    "        color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_negative_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "    plt.axis([1, 1e5, -1, 2])\n",
    "    plt.title('Coefficient path')\n",
    "    plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "    plt.ylabel('Coefficient value')\n",
    "    plt.xscale('log')\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
